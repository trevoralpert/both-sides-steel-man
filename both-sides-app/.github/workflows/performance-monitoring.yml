name: ðŸ“Š Performance Monitoring

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  push:
    branches: [ main ]
    paths-ignore:
      - '**.md'
      - '.github/workflows/**'
      - 'docs/**'
  workflow_dispatch:
    inputs:
      test_intensity:
        description: 'Test intensity level'
        required: false
        default: 'medium'
        type: choice
        options:
          - light
          - medium
          - heavy
      target_environment:
        description: 'Target environment'
        required: false
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  NODE_VERSION: '18'

jobs:
  # Job 1: Performance baseline measurement
  performance-baseline:
    name: ðŸ“ Performance Baseline
    runs-on: ubuntu-latest
    
    outputs:
      baseline-metrics: ${{ steps.baseline.outputs.metrics }}
      should-run-load-tests: ${{ steps.baseline.outputs.should-run-load-tests }}
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ðŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'yarn'
    
    - name: ðŸ”§ Enable Corepack
      run: corepack enable
    
    - name: ðŸ“¦ Install dependencies
      run: yarn install --frozen-lockfile
    
    - name: ðŸ—ï¸ Build application
      run: |
        echo "ðŸ—ï¸ Building application for performance testing..."
        yarn build
        
        # Extract build metrics
        BUILD_SIZE=$(du -sh .next 2>/dev/null | cut -f1 || echo "unknown")
        echo "ðŸ“¦ Build size: $BUILD_SIZE"
        echo "build-size=$BUILD_SIZE" >> $GITHUB_ENV
    
    - name: ðŸ“ Establish performance baseline
      id: baseline
      run: |
        echo "ðŸ“ Establishing performance baseline..."
        
        # Get current commit info
        COMMIT_SHA="${{ github.sha }}"
        COMMIT_DATE=$(git show -s --format=%ci $COMMIT_SHA)
        
        # Create baseline metrics
        cat > baseline-metrics.json << EOF
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "commit": "$COMMIT_SHA",
          "commit_date": "$COMMIT_DATE",
          "build_size": "${{ env.build-size }}",
          "environment": "${{ github.event.inputs.target_environment || 'staging' }}",
          "test_intensity": "${{ github.event.inputs.test_intensity || 'medium' }}",
          "metrics": {
            "build_time": "TBD",
            "bundle_size": "TBD",
            "lighthouse_score": "TBD",
            "load_test_results": "TBD"
          }
        }
        EOF
        
        # Output for next jobs
        echo "metrics<<EOF" >> $GITHUB_OUTPUT
        cat baseline-metrics.json >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT
        
        # Determine if we should run load tests
        if [[ "${{ github.event_name }}" == "schedule" ]] || 
           [[ "${{ github.event_name }}" == "workflow_dispatch" ]] ||
           [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
          echo "should-run-load-tests=true" >> $GITHUB_OUTPUT
          echo "ðŸš€ Load tests will be executed"
        else
          echo "should-run-load-tests=false" >> $GITHUB_OUTPUT
          echo "â­ï¸ Load tests skipped for this trigger"
        fi
    
    - name: ðŸ“Š Upload baseline metrics
      uses: actions/upload-artifact@v4
      with:
        name: baseline-metrics
        path: baseline-metrics.json
        retention-days: 30

  # Job 2: Lighthouse performance audit
  lighthouse-audit:
    name: ðŸ® Lighthouse Audit
    runs-on: ubuntu-latest
    needs: performance-baseline
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ðŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'yarn'
    
    - name: ðŸ”§ Enable Corepack
      run: corepack enable
    
    - name: ðŸ“¦ Install dependencies
      run: yarn install --frozen-lockfile
    
    - name: ðŸ—ï¸ Build application
      run: yarn build
    
    - name: ðŸš€ Start application server
      run: |
        echo "ðŸš€ Starting Next.js server..."
        yarn start &
        SERVER_PID=$!
        echo "server-pid=$SERVER_PID" >> $GITHUB_ENV
        
        # Wait for server to be ready
        echo "â³ Waiting for server to be ready..."
        for i in {1..30}; do
          if curl -s http://localhost:3000 > /dev/null; then
            echo "âœ… Server is ready"
            break
          fi
          echo "â³ Waiting... ($i/30)"
          sleep 2
        done
    
    - name: ðŸ® Run Lighthouse audit
      id: lighthouse
      run: |
        echo "ðŸ® Running Lighthouse performance audit..."
        
        # Install Lighthouse
        npm install -g lighthouse
        
        # Run Lighthouse audit
        lighthouse http://localhost:3000 \
          --output=json \
          --output=html \
          --output-path=./lighthouse-report \
          --chrome-flags="--headless --no-sandbox --disable-gpu" \
          --only-categories=performance,accessibility,best-practices,seo
        
        # Extract key metrics
        PERFORMANCE_SCORE=$(cat lighthouse-report.report.json | jq '.categories.performance.score * 100')
        ACCESSIBILITY_SCORE=$(cat lighthouse-report.report.json | jq '.categories.accessibility.score * 100')
        BEST_PRACTICES_SCORE=$(cat lighthouse-report.report.json | jq '.categories["best-practices"].score * 100')
        SEO_SCORE=$(cat lighthouse-report.report.json | jq '.categories.seo.score * 100')
        
        FCP=$(cat lighthouse-report.report.json | jq '.audits["first-contentful-paint"].numericValue')
        LCP=$(cat lighthouse-report.report.json | jq '.audits["largest-contentful-paint"].numericValue')
        CLS=$(cat lighthouse-report.report.json | jq '.audits["cumulative-layout-shift"].numericValue')
        
        echo "ðŸ“Š Lighthouse Results:"
        echo "  Performance: $PERFORMANCE_SCORE"
        echo "  Accessibility: $ACCESSIBILITY_SCORE" 
        echo "  Best Practices: $BEST_PRACTICES_SCORE"
        echo "  SEO: $SEO_SCORE"
        echo "  FCP: ${FCP}ms"
        echo "  LCP: ${LCP}ms"
        echo "  CLS: $CLS"
        
        # Output metrics
        echo "performance-score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT
        echo "accessibility-score=$ACCESSIBILITY_SCORE" >> $GITHUB_OUTPUT
        echo "best-practices-score=$BEST_PRACTICES_SCORE" >> $GITHUB_OUTPUT
        echo "seo-score=$SEO_SCORE" >> $GITHUB_OUTPUT
        echo "fcp=$FCP" >> $GITHUB_OUTPUT
        echo "lcp=$LCP" >> $GITHUB_OUTPUT
        echo "cls=$CLS" >> $GITHUB_OUTPUT
    
    - name: ðŸ›‘ Stop server
      if: always()
      run: |
        if [ -n "${{ env.server-pid }}" ]; then
          kill ${{ env.server-pid }} || true
          echo "ðŸ›‘ Server stopped"
        fi
    
    - name: ðŸ“Š Upload Lighthouse reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: lighthouse-reports
        path: |
          lighthouse-report.report.html
          lighthouse-report.report.json
        retention-days: 30

  # Job 3: Load testing execution
  load-testing:
    name: âš¡ Load Testing
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: needs.performance-baseline.outputs.should-run-load-tests == 'true'
    
    strategy:
      fail-fast: false
      matrix:
        test-type: [websocket, api]
        include:
          - test-type: websocket
            description: "WebSocket Performance"
            command: "load-test:websocket"
          - test-type: api
            description: "API Throughput"
            command: "load-test:api"
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ðŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'yarn'
    
    - name: ðŸ”§ Enable Corepack
      run: corepack enable
    
    - name: ðŸ“¦ Install dependencies
      run: yarn install --frozen-lockfile
    
    - name: âš¡ Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
    
    - name: ðŸš€ Run ${{ matrix.description }}
      id: load-test
      run: |
        echo "âš¡ Running ${{ matrix.description }}..."
        
        # Set environment variables
        export BASE_URL="${{ secrets.LOAD_TEST_BASE_URL || 'https://both-sides-demo.vercel.app' }}"
        export API_URL="${{ secrets.LOAD_TEST_API_URL || 'https://both-sides-demo.vercel.app/api' }}"
        export WS_URL="${{ secrets.LOAD_TEST_WS_URL || 'wss://both-sides-demo.vercel.app' }}"
        export ENVIRONMENT="performance-monitoring"
        export TEST_INTENSITY="${{ github.event.inputs.test_intensity || 'medium' }}"
        
        # Run load test with timeout
        timeout 600 yarn ${{ matrix.command }} || {
          echo "âš ï¸ Load test completed with timeout or warnings"
          echo "test-status=warning" >> $GITHUB_OUTPUT
          exit 0
        }
        
        echo "âœ… ${{ matrix.description }} completed successfully"
        echo "test-status=success" >> $GITHUB_OUTPUT
    
    - name: ðŸ“Š Upload load test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: load-test-results-${{ matrix.test-type }}
        path: src/__tests__/load-testing/reports/
        retention-days: 30

  # Job 4: Performance regression analysis
  regression-analysis:
    name: ðŸ“ˆ Regression Analysis
    runs-on: ubuntu-latest
    needs: [performance-baseline, lighthouse-audit, load-testing]
    if: always()
    
    steps:
    - name: ðŸ“¥ Checkout code
      uses: actions/checkout@v4
    
    - name: ðŸ“Š Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: performance-artifacts/
    
    - name: ðŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'yarn'
    
    - name: ðŸ”§ Enable Corepack
      run: corepack enable
    
    - name: ðŸ“¦ Install dependencies
      run: yarn install --frozen-lockfile
    
    - name: ðŸ“ˆ Analyze performance trends
      id: analysis
      run: |
        echo "ðŸ“ˆ Analyzing performance trends..."
        
        # Create performance summary
        cat > performance-summary.json << EOF
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "commit": "${{ github.sha }}",
          "environment": "${{ github.event.inputs.target_environment || 'staging' }}",
          "lighthouse": {
            "performance": ${{ needs.lighthouse-audit.outputs.performance-score || 0 }},
            "accessibility": ${{ needs.lighthouse-audit.outputs.accessibility-score || 0 }},
            "best_practices": ${{ needs.lighthouse-audit.outputs.best-practices-score || 0 }},
            "seo": ${{ needs.lighthouse-audit.outputs.seo-score || 0 }},
            "fcp": ${{ needs.lighthouse-audit.outputs.fcp || 0 }},
            "lcp": ${{ needs.lighthouse-audit.outputs.lcp || 0 }},
            "cls": ${{ needs.lighthouse-audit.outputs.cls || 0 }}
          },
          "load_tests": {
            "websocket_status": "TBD",
            "api_status": "TBD"
          }
        }
        EOF
        
        echo "ðŸ“Š Performance Summary:"
        cat performance-summary.json | jq '.'
        
        # Check for performance regressions
        PERFORMANCE_SCORE=${{ needs.lighthouse-audit.outputs.performance-score || 0 }}
        
        if (( $(echo "$PERFORMANCE_SCORE < 80" | bc -l) )); then
          echo "regression-detected=true" >> $GITHUB_OUTPUT
          echo "regression-type=lighthouse-performance" >> $GITHUB_OUTPUT
          echo "âš ï¸ Performance regression detected: Lighthouse score below 80"
        else
          echo "regression-detected=false" >> $GITHUB_OUTPUT
          echo "âœ… No significant performance regressions detected"
        fi
    
    - name: ðŸ“Š Upload performance summary
      uses: actions/upload-artifact@v4
      with:
        name: performance-summary
        path: performance-summary.json
        retention-days: 90

  # Job 5: Performance dashboard update
  dashboard-update:
    name: ðŸ“Š Dashboard Update
    runs-on: ubuntu-latest
    needs: [performance-baseline, lighthouse-audit, load-testing, regression-analysis]
    if: always()
    
    steps:
    - name: ðŸ“Š Prepare dashboard data
      run: |
        echo "ðŸ“Š Preparing performance dashboard data..."
        
        # Create dashboard payload
        cat > dashboard-data.json << EOF
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "commit": "${{ github.sha }}",
          "branch": "${{ github.ref_name }}",
          "workflow_run_id": "${{ github.run_id }}",
          "metrics": {
            "lighthouse_performance": ${{ needs.lighthouse-audit.outputs.performance-score || 0 }},
            "lighthouse_accessibility": ${{ needs.lighthouse-audit.outputs.accessibility-score || 0 }},
            "lighthouse_fcp": ${{ needs.lighthouse-audit.outputs.fcp || 0 }},
            "lighthouse_lcp": ${{ needs.lighthouse-audit.outputs.lcp || 0 }},
            "lighthouse_cls": ${{ needs.lighthouse-audit.outputs.cls || 0 }},
            "regression_detected": ${{ needs.regression-analysis.outputs.regression-detected || false }}
          },
          "status": {
            "lighthouse": "${{ needs.lighthouse-audit.result }}",
            "load_testing": "${{ needs.load-testing.result }}",
            "overall": "${{ needs.regression-analysis.outputs.regression-detected == 'true' && 'warning' || 'success' }}"
          }
        }
        EOF
        
        echo "ðŸ“Š Dashboard data prepared:"
        cat dashboard-data.json | jq '.'
    
    - name: ðŸ“¡ Send to monitoring system
      if: ${{ env.MONITORING_WEBHOOK_URL != '' }}
      env:
        MONITORING_WEBHOOK_URL: ${{ secrets.MONITORING_WEBHOOK_URL }}
      run: |
        echo "ðŸ“¡ Sending data to monitoring system..."
        
        curl -X POST "${{ secrets.MONITORING_WEBHOOK_URL }}" \
          -H "Content-Type: application/json" \
          -d @dashboard-data.json || {
          echo "âš ï¸ Failed to send data to monitoring system"
        }

  # Job 6: Notifications and alerts
  notifications:
    name: ðŸ“¢ Notifications
    runs-on: ubuntu-latest
    needs: [performance-baseline, lighthouse-audit, load-testing, regression-analysis]
    if: always()
    
    steps:
    - name: ðŸ“Š Calculate notification status
      id: status
      run: |
        LIGHTHOUSE_STATUS="${{ needs.lighthouse-audit.result }}"
        LOAD_TEST_STATUS="${{ needs.load-testing.result }}"
        REGRESSION_DETECTED="${{ needs.regression-analysis.outputs.regression-detected }}"
        
        if [[ "$REGRESSION_DETECTED" == "true" ]]; then
          echo "status=warning" >> $GITHUB_OUTPUT
          echo "message=âš ï¸ Performance regression detected" >> $GITHUB_OUTPUT
          echo "color=warning" >> $GITHUB_OUTPUT
        elif [[ "$LIGHTHOUSE_STATUS" == "failure" ]] || [[ "$LOAD_TEST_STATUS" == "failure" ]]; then
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "message=âŒ Performance monitoring failed" >> $GITHUB_OUTPUT
          echo "color=danger" >> $GITHUB_OUTPUT
        else
          echo "status=success" >> $GITHUB_OUTPUT
          echo "message=âœ… Performance monitoring completed successfully" >> $GITHUB_OUTPUT
          echo "color=good" >> $GITHUB_OUTPUT
        fi
    
    - name: ðŸ“¢ Slack notification
      if: always() && (steps.status.outputs.status != 'success' || github.event_name == 'schedule')
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ steps.status.outputs.status }}
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        channel: '#performance-monitoring'
        username: 'Performance Monitor'
        icon_emoji: ':chart_with_upwards_trend:'
        text: |
          ${{ steps.status.outputs.message }}
          
          **Lighthouse Performance**: ${{ needs.lighthouse-audit.outputs.performance-score || 'N/A' }}
          **Environment**: ${{ github.event.inputs.target_environment || 'staging' }}
          **Commit**: ${{ github.sha }}
          **View Details**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
